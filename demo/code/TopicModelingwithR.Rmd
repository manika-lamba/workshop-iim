---
title: "Topic Modeling with R"
author: "Dr. Manika Lamba"
date: "August 26, 2024"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Research Questions

1.  **What are the topics?**\

2.  **How are they interrelated?**\

3.  **What is the effect discipline (social sci. vs computing) & year has on the topics?**\

## Part 1 (LDA)

[**Load the data**]{.underline}

Let's start by loading our packages and loading the dataset.

```{r Load Data}
library(quanteda); library(tidyverse); library(RColorBrewer); library(quanteda.textplots)

dataset <- read_csv("https://raw.githubusercontent.com/manika-lamba/rladies-urmia/main/articles-sample.csv")

#create corpus
myCorpus <- corpus(dataset$Abstract)
```

[**Create the dfm (pre-processing)**]{.underline}

First, we need to create a dfm (document-feature matrix) and remove a standard list of English stop words.

We'll also remove sparse terms using the `trim` function.

```{r Pre-Processing}


dfm <- dfm(myCorpus,
           remove = c(stopwords("english")),
           ngrams=1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)

vdfm <- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs
```

Let's explore the top 50 words.

```{r Top Features2}
topfeatures(vdfm, n = 50)
```

Let's plot two word clouds: one with the raw term frequencies and one with TF-IDF.

```{r Word Clouds, warning=FALSE}
textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "Raw Counts")
textplot_wordcloud(dfm_tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
```

Let's now create a dendogram to get an idea of how the words are clustering.

```{r Clustering}
numWords <- 50

wordDfm <- dfm_sort(dfm_weight(vdfm, scheme = 'count'))

wordDfm <- t(wordDfm)[1:numWords,]  # keep the top numWords words
wordDistMat <- dist(wordDfm)
wordCluster <- hclust(wordDistMat)
plot(wordCluster, xlab="", main="TF-IDF Frequency weighting (First 50 Words)")
```

[**Topic Modeling (LDA)**]{.underline}

For the first part, we're going to use the `topicmodels` package to run LDA.

We're going to run Gibbs sampling which is a simulation based approach to LDA. There are multiple parameters we need to set.

The most important parameter is the number of topics. Usually, for your first time running topic modeling, there isn't a perfect number to start with. This is ok! Usually starting with 10 (hundred of documents) to 50 (tens of thousands of documents). Let's start with 20.

The second important parameter is the number of iterations. We'll set this as 500.

```{r LDA}
library(topicmodels)

# we now export to a format that we can run the topic model with
dtm <- convert(vdfm, to="topicmodels")

# estimate LDA with K topics
K <- 20
lda <- LDA(dtm, k = K, method = "Gibbs", 
                control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
```

[**Visualizations Example: LDAVis**]{.underline}

To explore our results, we'll use a Shiny-based interactive visualization called LDAvis. This has been prebuilt as a R package (FYI it's also available in Python).

In order to use it, we'll need to convert our model results (in the `lda` object) to a json object that LDAVis requires as its input.

```{r LDAVis}
#Create Json for LDAVis
library(LDAvis)

topicmodels2LDAvis <- function(x, ...){
    post <- topicmodels::posterior(x)
    if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
    mat <- x@wordassignments
    LDAvis::createJSON(
        phi = post[["terms"]], 
        theta = post[["topics"]],
        vocab = colnames(post[["terms"]]),
        doc.length = slam::row_sums(mat, na.rm = TRUE),
        term.frequency = slam::col_sums(mat, na.rm = TRUE)
    )
}


result <- LDA(dtm, 5)
serVis(topicmodels2LDAvis(result))
```

Let's view the topics.

```{r}
term <- terms(lda, 10)
term
```

Visualize the topics

```{r}
library(tidytext)
library(tidyverse)
library(ggplot2)
topics <- tidy(lda, matrix = "beta")

top_terms <-
  topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18)) +
  labs(title = "LDA Result", caption= "Top Terms") +
  ylab("") +
  xlab("") +
  coord_flip()
```

Like topics are probability distribution of words, in LDA documents are probability distributions of topics.

Accordingly, we can rank the documents (papers) by how much they rank for each topic. In other words,

First, let's extract the document-topic probability matrix.

```{r Doc-Topic Matrix}
# to get topic probabilities per document
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics)
probtopics <- probtopics
colnames(probtopics) <- paste("Topic",1:K)
```

Next, let's find the most representative document for Topic 1.

```{r Representative Docs}
filter.topic <- "Topic 1"

row <- order(-probtopics[,filter.topic])[1]
dataset$Abstract[row]
```

Save docstotopics

```{r}
lda.topics <- as.matrix(topics(lda))
write.csv(lda.topics,file=paste("docstotopics",K,"DocsToTopics.csv"))
```

Save topictoterms

```{r}
lda.terms <- as.matrix(terms(lda,5))
lda.terms[1:5,]

write.csv(lda.terms,file=paste("topicstoterms",K,"TopicsToTerms.csv"))

```

## Part 2 (CTM)

[**Load the data**]{.underline}

Let's reload our packages and dataset (no need to reload if you have it saved from part 1).

[**Create the dfm (pre-processing)**]{.underline}

This time, let's remove a pre-created list of "generic" words to our original stop list. These are words that are research terms that do not tell much about the subject itself.

```{r Pre-Processing2}
library("quanteda.textstats")

stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")

dfm <- dfm(myCorpus,
           remove = c(stopwords("english"), stopWords),
           ngrams= 1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)

vdfm <- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs
```

Let's explore the top 50 words.

```{r Top Features}
topfeatures(vdfm, n = 50)
```

Let's plot two word clouds: one with the raw term frequencies and one with TF-IDF.

```{r Word Clouds2, warning=FALSE}
textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "Raw Counts")
textplot_wordcloud(dfm_tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, "Dark2"), 
     random.order = F, rot.per=0.1, max.words=250, main = "TF-IDF")
```

[**Correlated Topic Model (CTM) with stm package**]{.underline}

For this part (and the next), we're going to use the `stm` package.

```{r Load package}
library(stm)

# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm")
```

Unlike the `topicmodels` packages, `stm` has built in features to help analysts reduce sparse terms (minDoc or minCount).

```{r Sparse Terms, fig.height=4}
plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))

out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)
```

This time, let's consider running a 40 topic model. The code simply loads the file. You can run the model (which will take several minutes) by uncommenting out the code.

```{r Model Fit}

k <- 40

load(file = "./ctmfit.RData")

#ctmFit <- stm(out$documents, out$vocab, K = k,
#              max.em.its = 150, data = out$meta, init.type #=               "Spectral", seed = 300)

#save(ctmFit, file = "./ctmfit.RData")
```

[**Exploring the results through `stm`'s visualizations**]{.underline}

Let's explore the topics.

```{r, fig.height=10, fig.width=8}
plot(ctmFit, 
         type = "summary", 
         xlim = c(0,.16), 
         n = 5, 
         labeltype = "prob",
         main = "UNCC Research Topics", 
         text.cex = 0.8)
```

There are a lot of static visualizations we can explore. We'll use the `plot.STM` function.

This function provides four different types of plots. Each can be selected using its name for the `type` parameter.

The four plots are:

1.  `summary` - plots topic proportions and names.

2.  `labels` - plots the top words for a specific topic.

3.  `perspectives` - compares two topics' words.

4.  `hist` - a histogram of the expected topic proportions across documents for a topic.

Let's examine one of the topics to interpret its meaning. Let's consider topic 25 using the `labels` type.

```{r}
plot(ctmFit, # model results
         type = "labels", # type of plot
         labeltype="prob", # label type for the words
         n = 30, # number of words to show
         topics = 25, # the topic we've selected
         text.cex = 1.2, # this increases the font by 20% (1.2 = 120%)
         width = 50) # this increases the width of the box
```

This is clearly Education topics. But if we look back at the summary, there's also topic 26 with related terms.

We can alternatively use a different weighting scheme to focus on the words that are most distinctive for each topic.

For this, we'll use the `frex` labeltype. FREX stands for *frequent*-*exclusive* words, thus indicating words that are frequently used but exclusive to the topic.

```{r}
plot(ctmFit, 
         type = "labels", 
         labeltype="frex",
         n = 30, 
         topics = 25, 
         text.cex = 1.2, 
         width = 50)
```

Or we can use the "lift"...

```{r}
plot(ctmFit, 
         type = "labels", 
         labeltype="lift", 
         n = 30, 
         topics = 25, 
         text.cex = 1.2, 
         width = 50)
```

There isn't a "correct" approach. Each offers a unique perspective and knowing each one can help your full interpretation of a topic.

```{r}
topicNames <- labelTopics(ctmFit, n = 5)
topic <- data.frame(
  TopicNumber = 1:k,
  TopicProportions = colMeans(ctmFit$theta))
```


[**Semantic Coherence & Exclusivity**]{.underline}

A quick view is that there are two ways of measuring topic "interpretability": Semantic Coherence and Exclusivity.

Semantic coherence measures the consistency of the words used within the topic. Larger values are better and mean the topic is more consistent. Low values sometimes imply the topic may be composed of sub-topics.

Exclusivity measures how distinctive the top words are to that topic. For this, larger or smaller is not necessary better or worse, but indicates whether the topic is unique (high value) or broad (low value).

Let's plot this using the `topicQuality` function.

```{r fig.height = 8, fig.width = 8}
topicQuality(ctmFit, out$documents)
```

## Part 3 (STM)

[**Load the data**]{.underline}

Let's reload our packages and dataset (no need to reload if you have it saved from part 1).

```{r}

#create corpus
myCorpus <- corpus(dataset$Abstract)
docvars(myCorpus, field = "Subject") <- ifelse(dataset$College=="Computing and Informatics",
                                               "Computing","Social Science")
docvars(myCorpus, field = "Year") <- as.integer(dataset$Year)
```

[**Create the dfm (pre-processing)**]{.underline}

```{r}
stopWords <- c("can","use","uses","used","using","study","studies","first","second","third","also","across","results","result","resulted","may","however","one","two","three","four","five","among","well","within","many","related","i.e","e.g","find","finding","finds","found","increase","increases","increasing","increased","decreased","decrease","decreases","decreasing","propose","proposal","proposals","proposes","proposed","new","old","differ","differs","different","difference","differences","positive","negative","findings","reports","report","reported","state","states","article","articles","examines","examine","suggest","research","researches","researchers","need","needs","show","shows","association","associations","associated","discuss","discusses","discussed","will","likely","unlikely","paper","method","methods","methodology","compared","specifically","approach","impact","impacts","examine","examined","examines","includes","include","included","including","measure","measures","measured","analysis","analyze","analyses","complete","completes","completed","indicate","indicated","indicates","high","higher","low","lower","follow","follows","following","significant","significance","approach","approaches","approached","model","models","demonstrate","demonstrated","demonstrates","yet","best","worst","better","large","small","larger","smaller","several","few","much","less","given","via","long","short","often","years","along","whether","potential","significantly","influence","influenced","influences","develop","develops","developed","good","bad","based","p","group","groups","effect","affect","affects","effects","sample","samples","relationship","relationships","change","changes","m","k","conclusion","conclusions","present","presents")

dfm <- dfm(myCorpus,
           remove = c(stopwords("english"), stopWords),
           ngrams= 1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)
```

[**Structural Topic Model: Subject and Year**]{.underline}

For this part, we're going to use the `stm` package to introduce two covariates into our model: field (Computing or Social Science) and year.

```{r}
library(stm)

# use quanteda converter to convert our Dfm
stmdfm <- convert(dfm, to = "stm", docvars = docvars(myCorpus))
```

Unlike the `topicmodels` packages, `stm` has built in features to help analysts reduce sparse terms (minDoc or minCount).

```{r fig.height=4}
plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))

out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)
```

Let's run a 40 topic model. The code simply loads the file. You can run the model (which will take several minutes) by uncommenting out the code.

```{r}

k <- 40

load("./stmFit.RData")

#stmFit <- stm(out$documents, out$vocab, K = k, prevalence =~ s(Year) + Subject, 
#              max.em.its = 150, data = out$meta, init.type = "Spectral", seed = 300)

#save(stmFit, file = "./stmFit.RData")
```

Let's explore the topics.

```{r, fig.height=10, fig.width=8}
plot(stmFit, 
         type = "summary", 
         xlim = c(0,.16), 
         n = 5, 
         labeltype = "prob",
         main = "UNCC Research Topics", 
         text.cex = 0.8)
```

We can see almost identical topics -- this is a good sign. Our topics are "stable" across runs, even in this case after we added in two prevalent covariates (Year and Subject).

Let's save our topic information.

```{r}
topicNames <- labelTopics(stmFit, n = 5)
topic <- data.frame(
  TopicNumber = 1:k,
  TopicProportions = colMeans(stmFit$theta))
```

[**Exploring the effects of the covariates: Subject**]{.underline}

Next, we want to explore the effect of the covariates on the topic proportions.

First, we'll need to use the `estimateEffect` function to estimate this effect.

```{r}
prep <- estimateEffect(1:40 ~ Subject + s(Year), stmFit, meta = out$meta, uncertainty = "Global")
```

We can then use the `plot.estimateEffect` function to compare the effect of the "Subject" field (Computing or Social Science binary flag) on topic proportions (likelihood of the topic).

```{r fig.height = 10, fig.width = 8}
Result <- plot(
  prep,
  "Subject",
  method = "difference",
  cov.value1 = "Social Science",
  cov.value2 = "Computing",
  verbose.labels = F,
  ylab = "Expected Difference in Topic Probability by Subject (with 95% CI)",
  xlab = "More Likely Computing                           Not Significant                       More Likely Social Science",
  main = "Effect of Subject on Topic Prevelance for UNCC Research",
  xlim = c(-0.1, 0.1)
  )
```

Let's redo this plot but rank the topics.

```{r fig.height = 10, fig.width = 8}
# order based on Expected Topic Proportion
rank = order(unlist(Result$means))
topicRnk <- topic[rank, ]

plot(
  prep,
  "Subject",
  method = "difference",
  cov.value1 = "Social Science",
  cov.value2 = "Computing",
  verbose.labels = F,
  topics = topicRnk$TopicNumber,
  #labeltype = "custom",
  #custom.labels  = apply(topicNames$prob, 1, function(x) paste0(x, collapse = " + ")),
  ylab = "Expected Difference in Topic Probability by Subject (with 95% CI)",
  xlab = "More Likely Computing                           Not Significant                       More Likely Social Science",
  main = "Effect of Subject on Topic Prevelance for UNCC Research",
  xlim = c(-0.1, 0.1)
)
```

[**Effect of Time**]{.underline}

```{r}
# time
par(mfrow = c(1, 1), mar = c(4, 4, 2, 2))
i <- c(9, 18)
plot(
  prep,
  "Year",
  method = "continuous",
  topics = i,
  main = "Topics 9 and 18 by Year",
  printlegend = T,
  ylab = "Exp. Topic Prob",
  xlab = "Year",
  ylim = c(-0.01, 0.16)
)
```

[**Session Info**]{.underline}

```{r}
sessionInfo()
```

## Excercise

```{r, excercise=TRUE}

data <- read.csv("https://raw.githubusercontent.com/textmining-infopros/chapter4/master/4c_dataset.csv")
```

```{r, include=FALSE}
#create corpus
myCorpus <- corpus(data$Title)
```
