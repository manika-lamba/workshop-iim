<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Dr. Manika Lamba" />

<meta name="date" content="2024-08-26" />

<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>Topic Modeling with R</title>

<!-- HEAD_CONTENT -->

<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->



</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<div id="section-research-questions" class="section level2">
<h2>Research Questions</h2>
<ol style="list-style-type: decimal">
<li><p><strong>What are the topics?</strong><br />
</p></li>
<li><p><strong>How are they interrelated?</strong><br />
</p></li>
<li><p><strong>What is the effect discipline (social sci. vs computing)
&amp; year has on the topics?</strong><br />
</p></li>
</ol>
</div>
<div id="section-part-1-lda" class="section level2">
<h2>Part 1 (LDA)</h2>
<p><u><strong>Load the data</strong></u></p>
<p>Let’s start by loading our packages and loading the dataset.</p>
<pre class="r"><code>library(quanteda); library(tidyverse); library(RColorBrewer); library(quanteda.textplots)

dataset &lt;- read_csv(&quot;https://raw.githubusercontent.com/manika-lamba/rladies-urmia/main/articles-sample.csv&quot;)

#create corpus
myCorpus &lt;- corpus(dataset$Abstract)</code></pre>
<p><u><strong>Create the dfm (pre-processing)</strong></u></p>
<p>First, we need to create a dfm (document-feature matrix) and remove a
standard list of English stop words.</p>
<p>We’ll also remove sparse terms using the <code>trim</code>
function.</p>
<pre class="r"><code>dfm &lt;- dfm(myCorpus,
           remove = c(stopwords(&quot;english&quot;)),
           ngrams=1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)

vdfm &lt;- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs</code></pre>
<p>Let’s explore the top 50 words.</p>
<pre class="r"><code>topfeatures(vdfm, n = 50)</code></pre>
<pre><code>##          use        studi       result        model         data       effect 
##         3190         2305         1738         1635         1554         1321 
##          can     research      develop       provid      student       method 
##         1305         1291         1262         1224         1178         1066 
##      network       differ       system     approach     signific       examin 
##         1014         1005          951          932          926          916 
##      analysi      increas       health         find       social        paper 
##          892          880          873          869          869          838 
##        group       inform         time         base      present        relat 
##          785          772          771          770          769          762 
##       design         also       measur       associ        activ       propos 
##          761          754          731          728          724          723 
##         user      perform       includ      support       import      process 
##          722          721          715          698          675          671 
## relationship         show     particip      suggest         educ          may 
##          671          666          665          659          654          651 
##        level          two 
##          645          638</code></pre>
<p>Let’s plot two word clouds: one with the raw term frequencies and one
with TF-IDF.</p>
<pre class="r"><code>textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, &quot;Dark2&quot;), 
     random.order = F, rot.per=0.1, max.words=250, main = &quot;Raw Counts&quot;)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Word%20Clouds-1.png" width="624" /></p>
<pre class="r"><code>textplot_wordcloud(dfm_tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, &quot;Dark2&quot;), 
     random.order = F, rot.per=0.1, max.words=250, main = &quot;TF-IDF&quot;)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Word%20Clouds-2.png" width="624" /></p>
<p>Let’s now create a dendogram to get an idea of how the words are
clustering.</p>
<pre class="r"><code>numWords &lt;- 50

wordDfm &lt;- dfm_sort(dfm_weight(vdfm, scheme = &#39;count&#39;))

wordDfm &lt;- t(wordDfm)[1:numWords,]  # keep the top numWords words
wordDistMat &lt;- dist(wordDfm)
wordCluster &lt;- hclust(wordDistMat)
plot(wordCluster, xlab=&quot;&quot;, main=&quot;TF-IDF Frequency weighting (First 50 Words)&quot;)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Clustering-1.png" width="624" /></p>
<p><u><strong>Topic Modeling (LDA)</strong></u></p>
<p>For the first part, we’re going to use the <code>topicmodels</code>
package to run LDA.</p>
<p>We’re going to run Gibbs sampling which is a simulation based
approach to LDA. There are multiple parameters we need to set.</p>
<p>The most important parameter is the number of topics. Usually, for
your first time running topic modeling, there isn’t a perfect number to
start with. This is ok! Usually starting with 10 (hundred of documents)
to 50 (tens of thousands of documents). Let’s start with 20.</p>
<p>The second important parameter is the number of iterations. We’ll set
this as 500.</p>
<pre class="r"><code>library(topicmodels)

# we now export to a format that we can run the topic model with
dtm &lt;- convert(vdfm, to=&quot;topicmodels&quot;)

# estimate LDA with K topics
K &lt;- 20
lda &lt;- LDA(dtm, k = K, method = &quot;Gibbs&quot;, 
                control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))</code></pre>
<pre><code>## K = 20; V = 3787; M = 2879
## Sampling 600 iterations!
## Iteration 25 ...
## Iteration 50 ...
## Iteration 75 ...
## Iteration 100 ...
## Iteration 125 ...
## Iteration 150 ...
## Iteration 175 ...
## Iteration 200 ...
## Iteration 225 ...
## Iteration 250 ...
## Iteration 275 ...
## Iteration 300 ...
## Iteration 325 ...
## Iteration 350 ...
## Iteration 375 ...
## Iteration 400 ...
## Iteration 425 ...
## Iteration 450 ...
## Iteration 475 ...
## Iteration 500 ...
## Iteration 525 ...
## Iteration 550 ...
## Iteration 575 ...
## Iteration 600 ...
## Gibbs sampling completed!</code></pre>
<p><u><strong>Visualizations Example: LDAVis</strong></u></p>
<p>To explore our results, we’ll use a Shiny-based interactive
visualization called LDAvis. This has been prebuilt as a R package (FYI
it’s also available in Python).</p>
<p>In order to use it, we’ll need to convert our model results (in the
<code>lda</code> object) to a json object that LDAVis requires as its
input.</p>
<pre class="r"><code>#Create Json for LDAVis
library(LDAvis)

topicmodels2LDAvis &lt;- function(x, ...){
    post &lt;- topicmodels::posterior(x)
    if (ncol(post[[&quot;topics&quot;]]) &lt; 3) stop(&quot;The model must contain &gt; 2 topics&quot;)
    mat &lt;- x@wordassignments
    LDAvis::createJSON(
        phi = post[[&quot;terms&quot;]], 
        theta = post[[&quot;topics&quot;]],
        vocab = colnames(post[[&quot;terms&quot;]]),
        doc.length = slam::row_sums(mat, na.rm = TRUE),
        term.frequency = slam::col_sums(mat, na.rm = TRUE)
    )
}


result &lt;- LDA(dtm, 5)
serVis(topicmodels2LDAvis(result))</code></pre>
<p>Let’s view the topics.</p>
<pre class="r"><code>term &lt;- terms(lda, 10)
term</code></pre>
<pre><code>##       Topic 1   Topic 2    Topic 3    Topic 4        Topic 5    Topic 6    
##  [1,] &quot;network&quot; &quot;activ&quot;    &quot;effect&quot;   &quot;area&quot;         &quot;user&quot;     &quot;health&quot;   
##  [2,] &quot;propos&quot;  &quot;signific&quot; &quot;use&quot;      &quot;urban&quot;        &quot;system&quot;   &quot;care&quot;     
##  [3,] &quot;secur&quot;   &quot;physic&quot;   &quot;design&quot;   &quot;spatial&quot;      &quot;approach&quot; &quot;american&quot; 
##  [4,] &quot;can&quot;     &quot;increas&quot;  &quot;studi&quot;    &quot;region&quot;       &quot;evalu&quot;    &quot;examin&quot;   
##  [5,] &quot;paper&quot;   &quot;p&quot;        &quot;perform&quot;  &quot;use&quot;          &quot;applic&quot;   &quot;age&quot;      
##  [6,] &quot;problem&quot; &quot;group&quot;    &quot;result&quot;   &quot;chang&quot;        &quot;base&quot;     &quot;need&quot;     
##  [7,] &quot;effici&quot;  &quot;use&quot;      &quot;three&quot;    &quot;develop&quot;      &quot;inform&quot;   &quot;communiti&quot;
##  [8,] &quot;node&quot;    &quot;chang&quot;    &quot;strategi&quot; &quot;land&quot;         &quot;access&quot;   &quot;servic&quot;   
##  [9,] &quot;rout&quot;    &quot;level&quot;    &quot;improv&quot;   &quot;citi&quot;         &quot;provid&quot;   &quot;older&quot;    
## [10,] &quot;requir&quot;  &quot;result&quot;   &quot;includ&quot;   &quot;neighborhood&quot; &quot;can&quot;      &quot;women&quot;    
##       Topic 7    Topic 8   Topic 9   Topic 10 Topic 11     Topic 12  
##  [1,] &quot;gene&quot;     &quot;measur&quot;  &quot;social&quot;  &quot;one&quot;    &quot;use&quot;        &quot;research&quot;
##  [2,] &quot;identifi&quot; &quot;test&quot;    &quot;state&quot;   &quot;use&quot;    &quot;interact&quot;   &quot;work&quot;    
##  [3,] &quot;structur&quot; &quot;control&quot; &quot;cultur&quot;  &quot;make&quot;   &quot;present&quot;    &quot;support&quot; 
##  [4,] &quot;express&quot;  &quot;group&quot;   &quot;polici&quot;  &quot;inform&quot; &quot;can&quot;        &quot;organ&quot;   
##  [5,] &quot;sequenc&quot;  &quot;assess&quot;  &quot;public&quot;  &quot;import&quot; &quot;technolog&quot;  &quot;practic&quot; 
##  [6,] &quot;analysi&quot;  &quot;score&quot;   &quot;human&quot;   &quot;two&quot;    &quot;provid&quot;     &quot;meet&quot;    
##  [7,] &quot;protein&quot;  &quot;compar&quot;  &quot;context&quot; &quot;decis&quot;  &quot;environ&quot;    &quot;provid&quot;  
##  [8,] &quot;cell&quot;     &quot;valid&quot;   &quot;way&quot;     &quot;howev&quot;  &quot;process&quot;    &quot;develop&quot; 
##  [9,] &quot;genom&quot;    &quot;p&quot;       &quot;polit&quot;   &quot;action&quot; &quot;understand&quot; &quot;discuss&quot; 
## [10,] &quot;use&quot;      &quot;ankl&quot;    &quot;new&quot;     &quot;first&quot;  &quot;creat&quot;      &quot;articl&quot;  
##       Topic 13   Topic 14       Topic 15  Topic 16   Topic 17    Topic 18
##  [1,] &quot;data&quot;     &quot;relat&quot;        &quot;product&quot; &quot;studi&quot;    &quot;imag&quot;      &quot;method&quot;
##  [2,] &quot;model&quot;    &quot;behavior&quot;     &quot;valu&quot;    &quot;differ&quot;   &quot;featur&quot;    &quot;model&quot; 
##  [3,] &quot;analysi&quot;  &quot;relationship&quot; &quot;firm&quot;    &quot;examin&quot;   &quot;method&quot;    &quot;plan&quot;  
##  [4,] &quot;comput&quot;   &quot;posit&quot;        &quot;increas&quot; &quot;signific&quot; &quot;visual&quot;    &quot;base&quot;  
##  [5,] &quot;time&quot;     &quot;person&quot;       &quot;find&quot;    &quot;rate&quot;     &quot;similar&quot;   &quot;use&quot;   
##  [6,] &quot;problem&quot;  &quot;influenc&quot;     &quot;market&quot;  &quot;find&quot;     &quot;result&quot;    &quot;estim&quot; 
##  [7,] &quot;system&quot;   &quot;studi&quot;        &quot;effect&quot;  &quot;type&quot;     &quot;cluster&quot;   &quot;detect&quot;
##  [8,] &quot;collect&quot;  &quot;affect&quot;       &quot;result&quot;  &quot;level&quot;    &quot;object&quot;    &quot;track&quot; 
##  [9,] &quot;approach&quot; &quot;find&quot;         &quot;impact&quot;  &quot;factor&quot;   &quot;learn&quot;     &quot;can&quot;   
## [10,] &quot;generat&quot;  &quot;percept&quot;      &quot;also&quot;    &quot;higher&quot;   &quot;algorithm&quot; &quot;object&quot;
##       Topic 19    Topic 20    
##  [1,] &quot;patient&quot;   &quot;student&quot;   
##  [2,] &quot;associ&quot;    &quot;school&quot;    
##  [3,] &quot;risk&quot;      &quot;educ&quot;      
##  [4,] &quot;children&quot;  &quot;teacher&quot;   
##  [5,] &quot;use&quot;       &quot;program&quot;   
##  [6,] &quot;year&quot;      &quot;learn&quot;     
##  [7,] &quot;among&quot;     &quot;instruct&quot;  
##  [8,] &quot;method&quot;    &quot;practic&quot;   
##  [9,] &quot;treatment&quot; &quot;develop&quot;   
## [10,] &quot;studi&quot;     &quot;profession&quot;</code></pre>
<p>Visualize the topics</p>
<pre class="r"><code>library(tidytext)
library(tidyverse)
library(ggplot2)
topics &lt;- tidy(lda, matrix = &quot;beta&quot;)

top_terms &lt;-
  topics %&gt;%
  group_by(topic) %&gt;%
  top_n(5, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)


top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18)) +
  labs(title = &quot;LDA Result&quot;, caption= &quot;Top Terms&quot;) +
  ylab(&quot;&quot;) +
  xlab(&quot;&quot;) +
  coord_flip()</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-2-1.png" width="624" /></p>
<p>Like topics are probability distribution of words, in LDA documents
are probability distributions of topics.</p>
<p>Accordingly, we can rank the documents (papers) by how much they rank
for each topic. In other words,</p>
<p>First, let’s extract the document-topic probability matrix.</p>
<pre class="r"><code># to get topic probabilities per document
postlist &lt;- posterior(lda)
probtopics &lt;- data.frame(postlist$topics)
probtopics &lt;- probtopics
colnames(probtopics) &lt;- paste(&quot;Topic&quot;,1:K)</code></pre>
<p>Next, let’s find the most representative document for Topic 1.</p>
<pre class="r"><code>filter.topic &lt;- &quot;Topic 1&quot;

row &lt;- order(-probtopics[,filter.topic])[1]
dataset$Abstract[row]</code></pre>
<pre><code>## [1] &quot;In this paper, we investigate how to design energy-efficient localized routing in a large-scale three-dimensional (3D) wireless network. Several 3D localized routing protocols were proposed to seek either energy efficiency or delivery guarantee in 3D wireless networks. However, recent results [1, 2] showed that there is no deterministic localized routing algorithm that guarantees either delivery of packets or energy efficiency of its routes in 3D networks. In this paper, we focus on design of a simple localized routing method which can provide energy efficiency with high probability in a randomly deployed 3D network. In particular, we extend our previous routing method designed for 2D networks [3] to 3D networks. The proposed 3D routing method is a simple variation of 3D greedy routing and can guarantee energy efficiency of its paths with high probability in random 3D networks. We also study its asymptotic critical transmission radius to ensure the packet delivery with high probability in random 3D networks. Simulation results confirm our theoretical results.&quot;</code></pre>
<p>Save docstotopics</p>
<pre class="r"><code>lda.topics &lt;- as.matrix(topics(lda))
write.csv(lda.topics,file=paste(&quot;docstotopics&quot;,K,&quot;DocsToTopics.csv&quot;))</code></pre>
<p>Save topictoterms</p>
<pre class="r"><code>lda.terms &lt;- as.matrix(terms(lda,5))
lda.terms[1:5,]</code></pre>
<pre><code>##      Topic 1   Topic 2    Topic 3   Topic 4   Topic 5    Topic 6    Topic 7   
## [1,] &quot;network&quot; &quot;activ&quot;    &quot;effect&quot;  &quot;area&quot;    &quot;user&quot;     &quot;health&quot;   &quot;gene&quot;    
## [2,] &quot;propos&quot;  &quot;signific&quot; &quot;use&quot;     &quot;urban&quot;   &quot;system&quot;   &quot;care&quot;     &quot;identifi&quot;
## [3,] &quot;secur&quot;   &quot;physic&quot;   &quot;design&quot;  &quot;spatial&quot; &quot;approach&quot; &quot;american&quot; &quot;structur&quot;
## [4,] &quot;can&quot;     &quot;increas&quot;  &quot;studi&quot;   &quot;region&quot;  &quot;evalu&quot;    &quot;examin&quot;   &quot;express&quot; 
## [5,] &quot;paper&quot;   &quot;p&quot;        &quot;perform&quot; &quot;use&quot;     &quot;applic&quot;   &quot;age&quot;      &quot;sequenc&quot; 
##      Topic 8   Topic 9  Topic 10 Topic 11    Topic 12   Topic 13 
## [1,] &quot;measur&quot;  &quot;social&quot; &quot;one&quot;    &quot;use&quot;       &quot;research&quot; &quot;data&quot;   
## [2,] &quot;test&quot;    &quot;state&quot;  &quot;use&quot;    &quot;interact&quot;  &quot;work&quot;     &quot;model&quot;  
## [3,] &quot;control&quot; &quot;cultur&quot; &quot;make&quot;   &quot;present&quot;   &quot;support&quot;  &quot;analysi&quot;
## [4,] &quot;group&quot;   &quot;polici&quot; &quot;inform&quot; &quot;can&quot;       &quot;organ&quot;    &quot;comput&quot; 
## [5,] &quot;assess&quot;  &quot;public&quot; &quot;import&quot; &quot;technolog&quot; &quot;practic&quot;  &quot;time&quot;   
##      Topic 14       Topic 15  Topic 16   Topic 17  Topic 18 Topic 19  
## [1,] &quot;relat&quot;        &quot;product&quot; &quot;studi&quot;    &quot;imag&quot;    &quot;method&quot; &quot;patient&quot; 
## [2,] &quot;behavior&quot;     &quot;valu&quot;    &quot;differ&quot;   &quot;featur&quot;  &quot;model&quot;  &quot;associ&quot;  
## [3,] &quot;relationship&quot; &quot;firm&quot;    &quot;examin&quot;   &quot;method&quot;  &quot;plan&quot;   &quot;risk&quot;    
## [4,] &quot;posit&quot;        &quot;increas&quot; &quot;signific&quot; &quot;visual&quot;  &quot;base&quot;   &quot;children&quot;
## [5,] &quot;person&quot;       &quot;find&quot;    &quot;rate&quot;     &quot;similar&quot; &quot;use&quot;    &quot;use&quot;     
##      Topic 20 
## [1,] &quot;student&quot;
## [2,] &quot;school&quot; 
## [3,] &quot;educ&quot;   
## [4,] &quot;teacher&quot;
## [5,] &quot;program&quot;</code></pre>
<pre class="r"><code>write.csv(lda.terms,file=paste(&quot;topicstoterms&quot;,K,&quot;TopicsToTerms.csv&quot;))</code></pre>
</div>
<div id="section-part-2-ctm" class="section level2">
<h2>Part 2 (CTM)</h2>
<p><u><strong>Load the data</strong></u></p>
<p>Let’s reload our packages and dataset (no need to reload if you have
it saved from part 1).</p>
<p><u><strong>Create the dfm (pre-processing)</strong></u></p>
<p>This time, let’s remove a pre-created list of “generic” words to our
original stop list. These are words that are research terms that do not
tell much about the subject itself.</p>
<pre class="r"><code>library(&quot;quanteda.textstats&quot;)

stopWords &lt;- c(&quot;can&quot;,&quot;use&quot;,&quot;uses&quot;,&quot;used&quot;,&quot;using&quot;,&quot;study&quot;,&quot;studies&quot;,&quot;first&quot;,&quot;second&quot;,&quot;third&quot;,&quot;also&quot;,&quot;across&quot;,&quot;results&quot;,&quot;result&quot;,&quot;resulted&quot;,&quot;may&quot;,&quot;however&quot;,&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;,&quot;among&quot;,&quot;well&quot;,&quot;within&quot;,&quot;many&quot;,&quot;related&quot;,&quot;i.e&quot;,&quot;e.g&quot;,&quot;find&quot;,&quot;finding&quot;,&quot;finds&quot;,&quot;found&quot;,&quot;increase&quot;,&quot;increases&quot;,&quot;increasing&quot;,&quot;increased&quot;,&quot;decreased&quot;,&quot;decrease&quot;,&quot;decreases&quot;,&quot;decreasing&quot;,&quot;propose&quot;,&quot;proposal&quot;,&quot;proposals&quot;,&quot;proposes&quot;,&quot;proposed&quot;,&quot;new&quot;,&quot;old&quot;,&quot;differ&quot;,&quot;differs&quot;,&quot;different&quot;,&quot;difference&quot;,&quot;differences&quot;,&quot;positive&quot;,&quot;negative&quot;,&quot;findings&quot;,&quot;reports&quot;,&quot;report&quot;,&quot;reported&quot;,&quot;state&quot;,&quot;states&quot;,&quot;article&quot;,&quot;articles&quot;,&quot;examines&quot;,&quot;examine&quot;,&quot;suggest&quot;,&quot;research&quot;,&quot;researches&quot;,&quot;researchers&quot;,&quot;need&quot;,&quot;needs&quot;,&quot;show&quot;,&quot;shows&quot;,&quot;association&quot;,&quot;associations&quot;,&quot;associated&quot;,&quot;discuss&quot;,&quot;discusses&quot;,&quot;discussed&quot;,&quot;will&quot;,&quot;likely&quot;,&quot;unlikely&quot;,&quot;paper&quot;,&quot;method&quot;,&quot;methods&quot;,&quot;methodology&quot;,&quot;compared&quot;,&quot;specifically&quot;,&quot;approach&quot;,&quot;impact&quot;,&quot;impacts&quot;,&quot;examine&quot;,&quot;examined&quot;,&quot;examines&quot;,&quot;includes&quot;,&quot;include&quot;,&quot;included&quot;,&quot;including&quot;,&quot;measure&quot;,&quot;measures&quot;,&quot;measured&quot;,&quot;analysis&quot;,&quot;analyze&quot;,&quot;analyses&quot;,&quot;complete&quot;,&quot;completes&quot;,&quot;completed&quot;,&quot;indicate&quot;,&quot;indicated&quot;,&quot;indicates&quot;,&quot;high&quot;,&quot;higher&quot;,&quot;low&quot;,&quot;lower&quot;,&quot;follow&quot;,&quot;follows&quot;,&quot;following&quot;,&quot;significant&quot;,&quot;significance&quot;,&quot;approach&quot;,&quot;approaches&quot;,&quot;approached&quot;,&quot;model&quot;,&quot;models&quot;,&quot;demonstrate&quot;,&quot;demonstrated&quot;,&quot;demonstrates&quot;,&quot;yet&quot;,&quot;best&quot;,&quot;worst&quot;,&quot;better&quot;,&quot;large&quot;,&quot;small&quot;,&quot;larger&quot;,&quot;smaller&quot;,&quot;several&quot;,&quot;few&quot;,&quot;much&quot;,&quot;less&quot;,&quot;given&quot;,&quot;via&quot;,&quot;long&quot;,&quot;short&quot;,&quot;often&quot;,&quot;years&quot;,&quot;along&quot;,&quot;whether&quot;,&quot;potential&quot;,&quot;significantly&quot;,&quot;influence&quot;,&quot;influenced&quot;,&quot;influences&quot;,&quot;develop&quot;,&quot;develops&quot;,&quot;developed&quot;,&quot;good&quot;,&quot;bad&quot;,&quot;based&quot;,&quot;p&quot;,&quot;group&quot;,&quot;groups&quot;,&quot;effect&quot;,&quot;affect&quot;,&quot;affects&quot;,&quot;effects&quot;,&quot;sample&quot;,&quot;samples&quot;,&quot;relationship&quot;,&quot;relationships&quot;,&quot;change&quot;,&quot;changes&quot;,&quot;m&quot;,&quot;k&quot;,&quot;conclusion&quot;,&quot;conclusions&quot;,&quot;present&quot;,&quot;presents&quot;)

dfm &lt;- dfm(myCorpus,
           remove = c(stopwords(&quot;english&quot;), stopWords),
           ngrams= 1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)

vdfm &lt;- dfm_trim(dfm, min_count = 10, min_docfreq = 5)
# min_count = remove words used less than x
# min_docfreq = remove words used in less than x docs</code></pre>
<p>Let’s explore the top 50 words.</p>
<pre class="r"><code>topfeatures(vdfm, n = 50)</code></pre>
<pre><code>##      data    provid   student   network    system    health    social    inform 
##      1554      1224      1178      1014       951       873       869       772 
##      time    design     activ      user   perform   develop   support    import 
##       771       761       724       722       721       708       698       675 
##   process  particip      educ     level   control   patient     evalu    experi 
##       671       665       654       645       630       625       618       615 
##      imag   problem      test   practic    factor    effect     learn       set 
##       607       605       602       596       590       586       575       574 
##  identifi      work    school  interact   program  structur      rate    improv 
##       564       563       557       550       537       533       527       524 
##    assess  individu  behavior     manag      care    comput    object   teacher 
##       523       521       510       509       503       496       484       483 
##       age communiti 
##       482       478</code></pre>
<p>Let’s plot two word clouds: one with the raw term frequencies and one
with TF-IDF.</p>
<pre class="r"><code>textplot_wordcloud(vdfm,  scale=c(3.5, .75), colors=brewer.pal(8, &quot;Dark2&quot;), 
     random.order = F, rot.per=0.1, max.words=250, main = &quot;Raw Counts&quot;)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Word%20Clouds2-1.png" width="624" /></p>
<pre class="r"><code>textplot_wordcloud(dfm_tfidf(vdfm),  scale=c(3.5, .75), colors=brewer.pal(8, &quot;Dark2&quot;), 
     random.order = F, rot.per=0.1, max.words=250, main = &quot;TF-IDF&quot;)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Word%20Clouds2-2.png" width="624" /></p>
<p><u><strong>Correlated Topic Model (CTM) with stm
package</strong></u></p>
<p>For this part (and the next), we’re going to use the <code>stm</code>
package.</p>
<pre class="r"><code>library(stm)

# use quanteda converter to convert our Dfm
stmdfm &lt;- convert(dfm, to = &quot;stm&quot;)</code></pre>
<p>Unlike the <code>topicmodels</code> packages, <code>stm</code> has
built in features to help analysts reduce sparse terms (minDoc or
minCount).</p>
<pre class="r"><code>plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/Sparse%20Terms-1.png" width="624" /></p>
<pre class="r"><code>out &lt;- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)</code></pre>
<pre><code>## Removing 13662 of 16975 terms (21501 of 169976 tokens) due to frequency 
## Your corpus now has 2879 documents, 3313 terms and 148475 tokens.</code></pre>
<p>This time, let’s consider running a 40 topic model. The code simply
loads the file. You can run the model (which will take several minutes)
by uncommenting out the code.</p>
<pre class="r"><code>k &lt;- 40

load(file = &quot;./ctmfit.RData&quot;)

#ctmFit &lt;- stm(out$documents, out$vocab, K = k,
#              max.em.its = 150, data = out$meta, init.type #=               &quot;Spectral&quot;, seed = 300)

#save(ctmFit, file = &quot;./ctmfit.RData&quot;)</code></pre>
<p><u><strong>Exploring the results through <code>stm</code>’s
visualizations</strong></u></p>
<p>Let’s explore the topics.</p>
<pre class="r"><code>plot(ctmFit, 
         type = &quot;summary&quot;, 
         xlim = c(0,.16), 
         n = 5, 
         labeltype = &quot;prob&quot;,
         main = &quot;UNCC Research Topics&quot;, 
         text.cex = 0.8)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-5-1.png" width="768" /></p>
<p>There are a lot of static visualizations we can explore. We’ll use
the <code>plot.STM</code> function.</p>
<p>This function provides four different types of plots. Each can be
selected using its name for the <code>type</code> parameter.</p>
<p>The four plots are:</p>
<ol style="list-style-type: decimal">
<li><p><code>summary</code> - plots topic proportions and
names.</p></li>
<li><p><code>labels</code> - plots the top words for a specific
topic.</p></li>
<li><p><code>perspectives</code> - compares two topics’ words.</p></li>
<li><p><code>hist</code> - a histogram of the expected topic proportions
across documents for a topic.</p></li>
</ol>
<p>Let’s examine one of the topics to interpret its meaning. Let’s
consider topic 25 using the <code>labels</code> type.</p>
<pre class="r"><code>plot(ctmFit, # model results
         type = &quot;labels&quot;, # type of plot
         labeltype=&quot;prob&quot;, # label type for the words
         n = 30, # number of words to show
         topics = 25, # the topic we&#39;ve selected
         text.cex = 1.2, # this increases the font by 20% (1.2 = 120%)
         width = 50) # this increases the width of the box</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-6-1.png" width="624" /></p>
<p>This is clearly Education topics. But if we look back at the summary,
there’s also topic 26 with related terms.</p>
<p>We can alternatively use a different weighting scheme to focus on the
words that are most distinctive for each topic.</p>
<p>For this, we’ll use the <code>frex</code> labeltype. FREX stands for
<em>frequent</em>-<em>exclusive</em> words, thus indicating words that
are frequently used but exclusive to the topic.</p>
<pre class="r"><code>plot(ctmFit, 
         type = &quot;labels&quot;, 
         labeltype=&quot;frex&quot;,
         n = 30, 
         topics = 25, 
         text.cex = 1.2, 
         width = 50)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-7-1.png" width="624" /></p>
<p>Or we can use the “lift”…</p>
<pre class="r"><code>plot(ctmFit, 
         type = &quot;labels&quot;, 
         labeltype=&quot;lift&quot;, 
         n = 30, 
         topics = 25, 
         text.cex = 1.2, 
         width = 50)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-8-1.png" width="624" /></p>
<p>There isn’t a “correct” approach. Each offers a unique perspective
and knowing each one can help your full interpretation of a topic.</p>
<pre class="r"><code>topicNames &lt;- labelTopics(ctmFit, n = 5)
topic &lt;- data.frame(
  TopicNumber = 1:k,
  TopicProportions = colMeans(ctmFit$theta))</code></pre>
<p><u><strong>Semantic Coherence &amp; Exclusivity</strong></u></p>
<p>A quick view is that there are two ways of measuring topic
“interpretability”: Semantic Coherence and Exclusivity.</p>
<p>Semantic coherence measures the consistency of the words used within
the topic. Larger values are better and mean the topic is more
consistent. Low values sometimes imply the topic may be composed of
sub-topics.</p>
<p>Exclusivity measures how distinctive the top words are to that topic.
For this, larger or smaller is not necessary better or worse, but
indicates whether the topic is unique (high value) or broad (low
value).</p>
<p>Let’s plot this using the <code>topicQuality</code> function.</p>
<pre class="r"><code>topicQuality(ctmFit, out$documents)</code></pre>
<pre><code>##  [1]  -92.41479 -107.67684  -88.44682  -91.85185  -83.92817 -105.99933
##  [7] -114.74823 -146.13330 -110.39519  -85.13465 -105.37255 -131.92169
## [13]  -93.70184 -119.06033  -97.20186 -145.60405 -115.28449 -112.41323
## [19] -156.72703  -96.39655  -92.20963 -115.21807  -95.09244 -125.87693
## [25] -140.72678 -119.37446  -85.46070  -85.62510  -97.98590 -126.89315
## [31] -115.03251 -117.76735  -80.26802 -101.94768 -122.71983 -119.36771
## [37]  -85.15481 -126.61498 -139.75354 -165.17198
##  [1] 9.891367 9.893273 9.757869 9.935071 9.869311 9.879823 9.788510 9.875835
##  [9] 9.872405 9.752688 9.822799 9.845527 9.854328 9.855343 9.859240 9.884459
## [17] 9.839593 9.860232 9.946346 9.926610 9.859171 9.870145 9.809486 9.784943
## [25] 9.865181 9.883887 9.771241 9.778077 9.828670 9.872509 9.849202 9.718598
## [33] 9.906965 9.695825 9.837974 9.687017 9.855441 9.828882 9.892345 9.935393</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-10-1.png" width="768" /></p>
</div>
<div id="section-part-3-stm" class="section level2">
<h2>Part 3 (STM)</h2>
<p><u><strong>Load the data</strong></u></p>
<p>Let’s reload our packages and dataset (no need to reload if you have
it saved from part 1).</p>
<pre class="r"><code>#create corpus
myCorpus &lt;- corpus(dataset$Abstract)
docvars(myCorpus, field = &quot;Subject&quot;) &lt;- ifelse(dataset$College==&quot;Computing and Informatics&quot;,
                                               &quot;Computing&quot;,&quot;Social Science&quot;)
docvars(myCorpus, field = &quot;Year&quot;) &lt;- as.integer(dataset$Year)</code></pre>
<p><u><strong>Create the dfm (pre-processing)</strong></u></p>
<pre class="r"><code>stopWords &lt;- c(&quot;can&quot;,&quot;use&quot;,&quot;uses&quot;,&quot;used&quot;,&quot;using&quot;,&quot;study&quot;,&quot;studies&quot;,&quot;first&quot;,&quot;second&quot;,&quot;third&quot;,&quot;also&quot;,&quot;across&quot;,&quot;results&quot;,&quot;result&quot;,&quot;resulted&quot;,&quot;may&quot;,&quot;however&quot;,&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;,&quot;among&quot;,&quot;well&quot;,&quot;within&quot;,&quot;many&quot;,&quot;related&quot;,&quot;i.e&quot;,&quot;e.g&quot;,&quot;find&quot;,&quot;finding&quot;,&quot;finds&quot;,&quot;found&quot;,&quot;increase&quot;,&quot;increases&quot;,&quot;increasing&quot;,&quot;increased&quot;,&quot;decreased&quot;,&quot;decrease&quot;,&quot;decreases&quot;,&quot;decreasing&quot;,&quot;propose&quot;,&quot;proposal&quot;,&quot;proposals&quot;,&quot;proposes&quot;,&quot;proposed&quot;,&quot;new&quot;,&quot;old&quot;,&quot;differ&quot;,&quot;differs&quot;,&quot;different&quot;,&quot;difference&quot;,&quot;differences&quot;,&quot;positive&quot;,&quot;negative&quot;,&quot;findings&quot;,&quot;reports&quot;,&quot;report&quot;,&quot;reported&quot;,&quot;state&quot;,&quot;states&quot;,&quot;article&quot;,&quot;articles&quot;,&quot;examines&quot;,&quot;examine&quot;,&quot;suggest&quot;,&quot;research&quot;,&quot;researches&quot;,&quot;researchers&quot;,&quot;need&quot;,&quot;needs&quot;,&quot;show&quot;,&quot;shows&quot;,&quot;association&quot;,&quot;associations&quot;,&quot;associated&quot;,&quot;discuss&quot;,&quot;discusses&quot;,&quot;discussed&quot;,&quot;will&quot;,&quot;likely&quot;,&quot;unlikely&quot;,&quot;paper&quot;,&quot;method&quot;,&quot;methods&quot;,&quot;methodology&quot;,&quot;compared&quot;,&quot;specifically&quot;,&quot;approach&quot;,&quot;impact&quot;,&quot;impacts&quot;,&quot;examine&quot;,&quot;examined&quot;,&quot;examines&quot;,&quot;includes&quot;,&quot;include&quot;,&quot;included&quot;,&quot;including&quot;,&quot;measure&quot;,&quot;measures&quot;,&quot;measured&quot;,&quot;analysis&quot;,&quot;analyze&quot;,&quot;analyses&quot;,&quot;complete&quot;,&quot;completes&quot;,&quot;completed&quot;,&quot;indicate&quot;,&quot;indicated&quot;,&quot;indicates&quot;,&quot;high&quot;,&quot;higher&quot;,&quot;low&quot;,&quot;lower&quot;,&quot;follow&quot;,&quot;follows&quot;,&quot;following&quot;,&quot;significant&quot;,&quot;significance&quot;,&quot;approach&quot;,&quot;approaches&quot;,&quot;approached&quot;,&quot;model&quot;,&quot;models&quot;,&quot;demonstrate&quot;,&quot;demonstrated&quot;,&quot;demonstrates&quot;,&quot;yet&quot;,&quot;best&quot;,&quot;worst&quot;,&quot;better&quot;,&quot;large&quot;,&quot;small&quot;,&quot;larger&quot;,&quot;smaller&quot;,&quot;several&quot;,&quot;few&quot;,&quot;much&quot;,&quot;less&quot;,&quot;given&quot;,&quot;via&quot;,&quot;long&quot;,&quot;short&quot;,&quot;often&quot;,&quot;years&quot;,&quot;along&quot;,&quot;whether&quot;,&quot;potential&quot;,&quot;significantly&quot;,&quot;influence&quot;,&quot;influenced&quot;,&quot;influences&quot;,&quot;develop&quot;,&quot;develops&quot;,&quot;developed&quot;,&quot;good&quot;,&quot;bad&quot;,&quot;based&quot;,&quot;p&quot;,&quot;group&quot;,&quot;groups&quot;,&quot;effect&quot;,&quot;affect&quot;,&quot;affects&quot;,&quot;effects&quot;,&quot;sample&quot;,&quot;samples&quot;,&quot;relationship&quot;,&quot;relationships&quot;,&quot;change&quot;,&quot;changes&quot;,&quot;m&quot;,&quot;k&quot;,&quot;conclusion&quot;,&quot;conclusions&quot;,&quot;present&quot;,&quot;presents&quot;)

dfm &lt;- dfm(myCorpus,
           remove = c(stopwords(&quot;english&quot;), stopWords),
           ngrams= 1L,
           stem = F,
           remove_numbers = TRUE, 
           remove_punct = TRUE,
           remove_symbols = TRUE)</code></pre>
<p><u><strong>Structural Topic Model: Subject and Year</strong></u></p>
<p>For this part, we’re going to use the <code>stm</code> package to
introduce two covariates into our model: field (Computing or Social
Science) and year.</p>
<pre class="r"><code>library(stm)

# use quanteda converter to convert our Dfm
stmdfm &lt;- convert(dfm, to = &quot;stm&quot;, docvars = docvars(myCorpus))</code></pre>
<p>Unlike the <code>topicmodels</code> packages, <code>stm</code> has
built in features to help analysts reduce sparse terms (minDoc or
minCount).</p>
<pre class="r"><code>plotRemoved(stmdfm$documents, lower.thresh = seq(1, 100, by = 10))</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-14-1.png" width="624" /></p>
<pre class="r"><code>out &lt;- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)</code></pre>
<pre><code>## Removing 13662 of 16975 terms (21501 of 169976 tokens) due to frequency 
## Your corpus now has 2879 documents, 3313 terms and 148475 tokens.</code></pre>
<p>Let’s run a 40 topic model. The code simply loads the file. You can
run the model (which will take several minutes) by uncommenting out the
code.</p>
<pre class="r"><code>k &lt;- 40

load(&quot;./stmFit.RData&quot;)

#stmFit &lt;- stm(out$documents, out$vocab, K = k, prevalence =~ s(Year) + Subject, 
#              max.em.its = 150, data = out$meta, init.type = &quot;Spectral&quot;, seed = 300)

#save(stmFit, file = &quot;./stmFit.RData&quot;)</code></pre>
<p>Let’s explore the topics.</p>
<pre class="r"><code>plot(stmFit, 
         type = &quot;summary&quot;, 
         xlim = c(0,.16), 
         n = 5, 
         labeltype = &quot;prob&quot;,
         main = &quot;UNCC Research Topics&quot;, 
         text.cex = 0.8)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-16-1.png" width="768" /></p>
<p>We can see almost identical topics – this is a good sign. Our topics
are “stable” across runs, even in this case after we added in two
prevalent covariates (Year and Subject).</p>
<p>Let’s save our topic information.</p>
<pre class="r"><code>topicNames &lt;- labelTopics(stmFit, n = 5)
topic &lt;- data.frame(
  TopicNumber = 1:k,
  TopicProportions = colMeans(stmFit$theta))</code></pre>
<p><u><strong>Exploring the effects of the covariates:
Subject</strong></u></p>
<p>Next, we want to explore the effect of the covariates on the topic
proportions.</p>
<p>First, we’ll need to use the <code>estimateEffect</code> function to
estimate this effect.</p>
<pre class="r"><code>prep &lt;- estimateEffect(1:40 ~ Subject + s(Year), stmFit, meta = out$meta, uncertainty = &quot;Global&quot;)</code></pre>
<p>We can then use the <code>plot.estimateEffect</code> function to
compare the effect of the “Subject” field (Computing or Social Science
binary flag) on topic proportions (likelihood of the topic).</p>
<pre class="r"><code>Result &lt;- plot(
  prep,
  &quot;Subject&quot;,
  method = &quot;difference&quot;,
  cov.value1 = &quot;Social Science&quot;,
  cov.value2 = &quot;Computing&quot;,
  verbose.labels = F,
  ylab = &quot;Expected Difference in Topic Probability by Subject (with 95% CI)&quot;,
  xlab = &quot;More Likely Computing                           Not Significant                       More Likely Social Science&quot;,
  main = &quot;Effect of Subject on Topic Prevelance for UNCC Research&quot;,
  xlim = c(-0.1, 0.1)
  )</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-19-1.png" width="768" /></p>
<p>Let’s redo this plot but rank the topics.</p>
<pre class="r"><code># order based on Expected Topic Proportion
rank = order(unlist(Result$means))
topicRnk &lt;- topic[rank, ]

plot(
  prep,
  &quot;Subject&quot;,
  method = &quot;difference&quot;,
  cov.value1 = &quot;Social Science&quot;,
  cov.value2 = &quot;Computing&quot;,
  verbose.labels = F,
  topics = topicRnk$TopicNumber,
  #labeltype = &quot;custom&quot;,
  #custom.labels  = apply(topicNames$prob, 1, function(x) paste0(x, collapse = &quot; + &quot;)),
  ylab = &quot;Expected Difference in Topic Probability by Subject (with 95% CI)&quot;,
  xlab = &quot;More Likely Computing                           Not Significant                       More Likely Social Science&quot;,
  main = &quot;Effect of Subject on Topic Prevelance for UNCC Research&quot;,
  xlim = c(-0.1, 0.1)
)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-20-1.png" width="768" /></p>
<p><u><strong>Effect of Time</strong></u></p>
<pre class="r"><code># time
par(mfrow = c(1, 1), mar = c(4, 4, 2, 2))
i &lt;- c(9, 18)
plot(
  prep,
  &quot;Year&quot;,
  method = &quot;continuous&quot;,
  topics = i,
  main = &quot;Topics 9 and 18 by Year&quot;,
  printlegend = T,
  ylab = &quot;Exp. Topic Prob&quot;,
  xlab = &quot;Year&quot;,
  ylim = c(-0.01, 0.16)
)</code></pre>
<p><img src="TopicModelingwithR_files/figure-html/unnamed-chunk-21-1.png" width="624" /></p>
<p><u><strong>Session Info</strong></u></p>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.1.3 (2022-03-10)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22631)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.1252 
## [2] LC_CTYPE=English_United States.1252   
## [3] LC_MONETARY=English_United States.1252
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.1252    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] stm_1.3.6                 quanteda.textstats_0.95  
##  [3] tidytext_0.3.2            LDAvis_0.3.2             
##  [5] topicmodels_0.2-12        quanteda.textplots_0.94.1
##  [7] RColorBrewer_1.1-2        forcats_0.5.1            
##  [9] stringr_1.4.0             dplyr_1.1.2              
## [11] purrr_1.0.1               readr_2.1.2              
## [13] tidyr_1.2.0               tibble_3.2.1             
## [15] ggplot2_3.3.5             tidyverse_1.3.1          
## [17] quanteda_3.2.1            shiny_1.7.1              
## 
## loaded via a namespace (and not attached):
##  [1] colorspace_2.0-3    ellipsis_0.3.2      ISOcodes_2022.01.10
##  [4] modeltools_0.2-23   rprojroot_2.0.2     markdown_1.1       
##  [7] fs_1.5.2            rstudioapi_0.13     proxy_0.4-26       
## [10] farver_2.1.0        SnowballC_0.7.0     bit64_4.0.5        
## [13] fansi_1.0.2         lubridate_1.8.0     xml2_1.3.3         
## [16] splines_4.1.3       knitr_1.38          jsonlite_1.8.0     
## [19] broom_0.7.12        servr_0.24          dbplyr_2.1.1       
## [22] compiler_4.1.3      httr_1.4.2          backports_1.4.1    
## [25] assertthat_0.2.1    Matrix_1.4-0        fastmap_1.1.0      
## [28] cli_3.6.1           later_1.3.0         htmltools_0.5.2    
## [31] tools_4.1.3         NLP_0.2-1           gtable_0.3.0       
## [34] glue_1.6.2          reshape2_1.4.4      fastmatch_1.1-3    
## [37] Rcpp_1.0.10         slam_0.1-50         cellranger_1.1.0   
## [40] jquerylib_0.1.4     vctrs_0.6.1         RJSONIO_1.3-1.6    
## [43] xfun_0.30           stopwords_2.3       rvest_1.0.2        
## [46] nsyllable_1.0.1     mime_0.12           lifecycle_1.0.4    
## [49] scales_1.2.1        vroom_1.5.7         hms_1.1.1          
## [52] promises_1.2.0.1    parallel_4.1.3      yaml_2.3.5         
## [55] curl_4.3.2          sass_0.4.1          stringi_1.7.6      
## [58] highr_0.9           tokenizers_0.2.1    matrixStats_0.61.0 
## [61] rlang_1.1.0         pkgconfig_2.0.3     evaluate_0.15      
## [64] lattice_0.20-45     htmlwidgets_1.5.4   labeling_0.4.2     
## [67] bit_4.0.4           tidyselect_1.2.0    plyr_1.8.6         
## [70] magrittr_2.0.2      learnr_0.10.1       R6_2.5.1           
## [73] generics_0.1.2      DBI_1.1.2           pillar_1.9.0       
## [76] haven_2.4.3         withr_2.5.0         janeaustenr_0.1.5  
## [79] modelr_0.1.8        crayon_1.5.1        utf8_1.2.2         
## [82] tzdb_0.3.0          rmarkdown_2.13      grid_4.1.3         
## [85] readxl_1.3.1        data.table_1.14.2   reprex_2.0.1       
## [88] digest_0.6.29       xtable_1.8-4        tm_0.7-8           
## [91] httpuv_1.6.5        RcppParallel_5.1.5  stats4_4.1.3       
## [94] munsell_0.5.0       bslib_0.3.1</code></pre>
</div>
<div id="section-excercise" class="section level2">
<h2>Excercise</h2>
<pre class="r"><code>data &lt;- read.csv(&quot;https://raw.githubusercontent.com/textmining-infopros/chapter4/master/4c_dataset.csv&quot;)</code></pre>

<script type="application/shiny-prerendered" data-context="server-start">
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["header-attrs"]},{"type":"character","attributes":{},"value":["2.13"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pandoc"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["header-attrs.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["3.6.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/3.6.0"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery-3.6.0.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquerylib"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.1.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["3.6.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/3.6.0"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery-3.6.0.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquerylib"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.1.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.13"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","bit","bit64","broom","bslib","cellranger","cli","colorspace","compiler","crayon","curl","data.table","datasets","DBI","dbplyr","digest","dplyr","ellipsis","evaluate","fansi","farver","fastmap","fastmatch","forcats","fs","generics","ggplot2","glue","graphics","grDevices","grid","gtable","haven","highr","hms","htmltools","htmlwidgets","httpuv","httr","ISOcodes","janeaustenr","jquerylib","jsonlite","knitr","labeling","later","lattice","LDAvis","learnr","lifecycle","lubridate","magrittr","markdown","Matrix","matrixStats","methods","mime","modelr","modeltools","munsell","NLP","nsyllable","parallel","pillar","pkgconfig","plyr","promises","proxy","purrr","quanteda","quanteda.textplots","quanteda.textstats","R6","RColorBrewer","Rcpp","RcppParallel","readr","readxl","reprex","reshape2","RJSONIO","rlang","rmarkdown","rprojroot","rstudioapi","rvest","sass","scales","servr","shiny","slam","SnowballC","splines","stats","stats4","stm","stopwords","stringi","stringr","tibble","tidyr","tidyselect","tidytext","tidyverse","tm","tokenizers","tools","topicmodels","tzdb","utf8","utils","vctrs","vroom","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.4.1","4.1.3","4.0.4","4.0.5","0.7.12","0.3.1","1.1.0","3.6.1","2.0-3","4.1.3","1.5.1","4.3.2","1.14.2","4.1.3","1.1.2","2.1.1","0.6.29","1.1.2","0.3.2","0.15","1.0.2","2.1.0","1.1.0","1.1-3","0.5.1","1.5.2","0.1.2","3.3.5","1.6.2","4.1.3","4.1.3","4.1.3","0.3.0","2.4.3","0.9","1.1.1","0.5.2","1.5.4","1.6.5","1.4.2","2022.01.10","0.1.5","0.1.4","1.8.0","1.38","0.4.2","1.3.0","0.20-45","0.3.2","0.10.1","1.0.4","1.8.0","2.0.2","1.1","1.4-0","0.61.0","4.1.3","0.12","0.1.8","0.2-23","0.5.0","0.2-1","1.0.1","4.1.3","1.9.0","2.0.3","1.8.6","1.2.0.1","0.4-26","1.0.1","3.2.1","0.94.1","0.95","2.5.1","1.1-2","1.0.10","5.1.5","2.1.2","1.3.1","2.0.1","1.4.4","1.3-1.6","1.1.0","2.13","2.0.2","0.13","1.0.2","0.4.1","1.2.1","0.24","1.7.1","0.1-50","0.7.0","4.1.3","4.1.3","4.1.3","1.3.6","2.3","1.7.6","1.4.0","3.2.1","1.2.0","1.2.0","0.3.2","1.3.1","0.7-8","0.2.1","4.1.3","0.2-12","0.3.0","1.2.2","4.1.3","0.6.1","1.5.7","2.5.0","0.30","1.3.3","1.8-4","2.3.5"]}]}]}
</script>
<!--/html_preserve-->
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Topic Modeling with
R</h2>
<h4 class="author"><em>Dr. Manika Lamba</em></h4>
<h4 class="date"><em>August 26, 2024</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
